{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from model_llama3_2s import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"Llama3.1-8B-Instruct/params.json\", \"r\") as st_json:\n",
    "    params = json.load(st_json)\n",
    "params\n",
    "\n",
    "args = ModelArgs(**params)\n",
    "transformer = Transformer(args)\n",
    "\n",
    "model_pth = torch.load(\"Llama3.1-8B-Instruct/consolidated.00.pth\", map_location=\"cpu\", weights_only=True)\n",
    "transformer.load_state_dict(model_pth, strict=False)\n",
    "transformer.eval()\n",
    "\n",
    "from tokenizer import Tokenizer, ChatFormat\n",
    "tok = Tokenizer(\"Llama3.1-8B-Instruct/tokenizer.model\")\n",
    "formatter = ChatFormat(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs = [\n",
    "    [{\"role\": \"user\", \"content\": \"hello!ğŸ˜†\"}],\n",
    "]\n",
    "\n",
    "prompt_tokens = [\n",
    "    formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n",
    "]\n",
    "# prompt = torch.tensor(np.array(tok.encode(\"hello world!\", bos= True, eos= False))[None, :])\n",
    "prompt = torch.tensor(prompt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pad_id = tok.pad_id\n",
    "tokens = torch.full((1, 1000), pad_id, dtype=torch.long)\n",
    "\n",
    "for k, t in enumerate(prompt):\n",
    "    tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long)\n",
    "token_logprobs = torch.zeros_like(tokens, dtype=torch.float)\n",
    "\n",
    "prev_pos = 0\n",
    "eos_reached = torch.tensor([False] * 1)\n",
    "input_text_mask = tokens != pad_id\n",
    "\n",
    "temperature = 0\n",
    "stop_tokens = torch.tensor(list(tok.stop_tokens))\n",
    "prev_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = KVCache(\n",
    "    transformer.caches_shape, dtype=torch.float16\n",
    ")\n",
    "\n",
    "for cur_pos in range(len(prompt[0]), 800):\n",
    "    seqlen = tokens[:, prev_pos:cur_pos].size(1)\n",
    "\n",
    "    mask = torch.full((seqlen, seqlen), -1e9)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    mask = torch.hstack(\n",
    "        [torch.zeros((seqlen, prev_pos)), mask]\n",
    "    )[None, None, :, :]\n",
    "\n",
    "    logits = transformer.forward(tokens[:, prev_pos:cur_pos], mask, cache)\n",
    "    next_token = torch.argmax(logits[..., -1, :], dim=-1)\n",
    "\n",
    "    next_token = next_token.reshape(-1)\n",
    "    next_token = torch.where(\n",
    "        input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "    )\n",
    "    tokens[:, cur_pos] = next_token\n",
    "    \n",
    "    eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "        torch.isin(next_token, stop_tokens)\n",
    "    )\n",
    "    prev_pos = cur_pos\n",
    "    if all(eos_reached):\n",
    "        break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tok.decode(tokens[0, :prev_pos].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coreML_transformer = Llama_coreML(transformer= transformer)\n",
    "coreML_transformer.transformer.load_state_dict(transformer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del transformer\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids: torch.Tensor = torch.zeros((1, 5), dtype=torch.int32)\n",
    "causal_mask: torch.Tensor = torch.zeros((1, 1, 5, 5), dtype=torch.float32)\n",
    "\n",
    "traced_transformer = torch.jit.trace(coreML_transformer.eval(),  [input_ids, causal_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caches_shape = coreML_transformer.transformer.caches_shape\n",
    "del coreML_transformer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "import numpy as np\n",
    "ct.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_length = ct.RangeDim(lower_bound=1, upper_bound=2000, default=1)\n",
    "end_step_dim = ct.RangeDim(lower_bound=1, upper_bound=2000, default=1)\n",
    "inputs = [\n",
    "    ct.TensorType(shape=(1, query_length), dtype=np.int32, name=\"input_ids\"),\n",
    "    ct.TensorType(shape=(1, 1, query_length, end_step_dim), dtype=np.int32, name=\"causal_mask\"),\n",
    "]\n",
    "\n",
    "states = [\n",
    "    ct.StateType(\n",
    "        wrapped_type=ct.TensorType(shape=caches_shape, \n",
    "                                   dtype=np.float16, \n",
    "                                   ),\n",
    "        name=\"keyCache\",\n",
    "    ),\n",
    "    ct.StateType(\n",
    "        wrapped_type=ct.TensorType(shape=caches_shape, \n",
    "                                   dtype=np.float16, \n",
    "                                   ),\n",
    "        name=\"valueCache\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "outputs = [ct.TensorType(dtype=np.float32, name=\"logits\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlmodel_fp16 = ct.convert(\n",
    "    traced_transformer,\n",
    "    inputs=inputs,\n",
    "    states=states,\n",
    "    outputs=outputs,\n",
    "    minimum_deployment_target=ct.target.iOS18,\n",
    "    # skip_model_load=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del traced_transformer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block-wise quantize model weights to int4\n",
    "MODEL_ID: str = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "METADATA_TOKENIZER: str = \"tokenizer.model\"\n",
    "\n",
    "op_config = ct.optimize.coreml.OpLinearQuantizerConfig(\n",
    "    mode=\"linear_symmetric\",\n",
    "    dtype=\"int4\",\n",
    "    granularity=\"per_block\",\n",
    "    block_size=[1, 32],\n",
    ")\n",
    "\n",
    "config = ct.optimize.coreml.OptimizationConfig(global_config=op_config)\n",
    "mlmodel_int4 = ct.optimize.coreml.linear_quantize_weights(mlmodel_fp16, config=config)\n",
    "# mlmodel_int4._spec.description.metadata.userDefined.update({METADATA_TOKENIZER: MODEL_ID})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_int4.save(\"coreml_1B_INT4.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = mlmodel_fp16.make_state()  # ë£¨í”„ ë‚´ì—ì„œ ìƒíƒœ ì´ˆê¸°í™”\n",
    "index = 0\n",
    "c = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_tokens = torch.arange(1, 10)[None, :]\n",
    "c = seq_len = len(prompt_tokens[0])\n",
    "\n",
    "\n",
    "mask = torch.full((seq_len, seq_len), -1e9)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask = torch.hstack(\n",
    "    [torch.zeros((seq_len, 0)), mask]\n",
    ")[None, None, :, :]\n",
    "\n",
    "print(mask.shape)\n",
    "\n",
    "logits = coreML_transformer.forward(torch.tensor(prompt_tokens), mask)\n",
    "pre = torch.argmax(logits, -1)\n",
    "print(torch.argmax(logits, -1))\n",
    "\n",
    "a = []\n",
    "for i in range(len(prompt_tokens[0])+1, 200):\n",
    "    seq_len = len(pre)\n",
    "    c += 1\n",
    "\n",
    "    mask = torch.full((seq_len, seq_len), -1e9)\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    mask = torch.hstack(\n",
    "        [torch.zeros((seq_len, c)), mask]\n",
    "    )[None, None, :, :]\n",
    "    \n",
    "    # print(mask.shape)\n",
    "    # print(pre.shape)\n",
    "    logits = coreML_transformer.forward(pre[None, :], mask)\n",
    "    pre = torch.argmax(logits, -1)\n",
    "\n",
    "    a.append(int(pre[0]))\n",
    "print(a)\n",
    "print(tok.decode(a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_tokens = torch.arange(1, 10)[None, :]\n",
    "seq_len = len(prompt_tokens[0])\n",
    "\n",
    "mask = torch.full((seq_len, seq_len), -1e9)\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "mask = torch.hstack(\n",
    "    [torch.zeros((seq_len, seq_len)), mask]\n",
    ")[None, None, :, :]\n",
    "logits = traced_transformer.forward(torch.tensor(prompt_tokens), mask)\n",
    "print(torch.argmax(logits, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "\n",
    "c = seq_len = len(prompt_tokens[0])\n",
    "mask = np.full((seq_len, seq_len), -1e9)\n",
    "mask = np.triu(mask, k=1)\n",
    "mask = np.hstack(\n",
    "    [np.zeros((seq_len, 0)), mask]\n",
    ")[None, None, :, :]\n",
    "print(mask.shape)\n",
    "\n",
    "\n",
    "state = mlmodel_fp16.make_state()  # ë£¨í”„ ë‚´ì—ì„œ ìƒíƒœ ì´ˆê¸°í™”\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": np.array(prompt_tokens, dtype=np.int32),\n",
    "    \"causal_mask\": mask,\n",
    "}\n",
    "logits = mlmodel_fp16.predict(inputs, state = state)['logits']\n",
    "pre = np.argmax(logits, axis=-1)\n",
    "a.append(int(pre[0]))\n",
    "print(np.argmax(logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(prompt_tokens[0])+1, 200):\n",
    "    c += 1\n",
    "    seq_len = len(pre)\n",
    "    mask = np.full((seq_len, seq_len), -1e9)\n",
    "    mask = np.triu(mask, k=1)\n",
    "    mask = np.hstack(\n",
    "        [np.zeros((seq_len, c)), mask]\n",
    "    )[None, None, :, :]\n",
    "    # print(pre.mask)\n",
    "\n",
    "    inputs = {\n",
    "        \"input_ids\": np.array([pre], dtype=np.int32),\n",
    "        \"causal_mask\": mask,\n",
    "    }\n",
    "    preds = mlmodel_fp16.predict(inputs, state = state)\n",
    "    logits = preds['logits']\n",
    "    pre = np.argmax(logits, axis=-1)\n",
    "\n",
    "    a.append(int(pre[0]))\n",
    "    if int(pre[0]) in stop_tokens.tolist():\n",
    "        break\n",
    "print(a)\n",
    "print(tok.decode(prompt_tokens[0] + a))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlmodel_int4 = ct.models.MLModel(\"coreml_1B_INT4.mlpackage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_repetition_penalty(logits, generated_tokens, penalty):\n",
    "    \"\"\"\n",
    "    Repetition Penaltyë¥¼ ì ìš©í•œ logits ê°’ì„ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜.\n",
    "    \n",
    "    Parameters:\n",
    "    - logits: (numpy array) í˜„ì¬ ìŠ¤í…ì—ì„œì˜ logits ê°’\n",
    "    - generated_tokens: (list) ì´ì „ì— ìƒì„±ëœ í† í°ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
    "    - penalty: (float) íŒ¨ë„í‹° ê°’ (ë³´í†µ 1.0ë³´ë‹¤ í° ê°’ì„ ì„¤ì •)\n",
    "    \n",
    "    Returns:\n",
    "    - ìˆ˜ì •ëœ logits ê°’\n",
    "    \"\"\"\n",
    "    for token in set(generated_tokens):  # ì¤‘ë³µì„ ë°©ì§€í•˜ê¸° ìœ„í•´ set ì‚¬ìš©\n",
    "        if logits[..., token] > 0:\n",
    "            logits[..., token] /= penalty  # íŒ¨ë„í‹° ì ìš©\n",
    "        else:\n",
    "            logits[..., token] *= penalty  # ìŒìˆ˜ì¼ ê²½ìš° ë°˜ëŒ€ë¡œ ê³±í•¨\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def top_p_sampling(logits, p=0.9):\n",
    "    \"\"\"\n",
    "    Top-p ìƒ˜í”Œë§ì„ ì ìš©í•˜ì—¬ ë‹¤ìŒ í† í°ì„ ì„ íƒí•˜ëŠ” í•¨ìˆ˜.\n",
    "\n",
    "    Parameters:\n",
    "    - logits: (numpy array) shape=(1, seq_len, vocab)ì¸ logits ê°’\n",
    "    - p: (float) ëˆ„ì  í™•ë¥ ì˜ ê¸°ì¤€ (0 < p <= 1)\n",
    "\n",
    "    Returns:\n",
    "    - selected_token: (int) ì„ íƒëœ í† í°ì˜ ì¸ë±ìŠ¤\n",
    "    \"\"\"\n",
    "    # logitsë¥¼ softmaxë¥¼ ì‚¬ìš©í•˜ì—¬ í™•ë¥ ë¡œ ë³€í™˜\n",
    "    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True)\n",
    "    \n",
    "    # ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ ë‹¨ì–´ë“¤ì˜ ì¸ë±ìŠ¤ë¥¼ ì •ë ¬\n",
    "    sorted_indices = np.argsort(probabilities[0])[::-1]\n",
    "    sorted_probabilities = probabilities[0][sorted_indices]\n",
    "    \n",
    "    # ëˆ„ì  í™•ë¥  ê³„ì‚°\n",
    "    cumulative_probs = np.cumsum(sorted_probabilities)\n",
    "    \n",
    "    # pë³´ë‹¤ ì‘ì€ ë‹¨ì–´ë“¤ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°\n",
    "    cutoff_index = np.searchsorted(cumulative_probs, p)\n",
    "    \n",
    "    # top-p í›„ë³´ ë‹¨ì–´ì™€ ê·¸ í™•ë¥ \n",
    "    top_p_indices = sorted_indices[:cutoff_index + 1]\n",
    "    top_p_probs = sorted_probabilities[:cutoff_index + 1]\n",
    "\n",
    "    # top-p í›„ë³´ë“¤ ì¤‘ì—ì„œ ë¬´ì‘ìœ„ë¡œ ì„ íƒ\n",
    "    selected_token = np.random.choice(top_p_indices, p=top_p_probs/top_p_probs.sum())\n",
    "    \n",
    "    return np.array([selected_token])\n",
    "\n",
    "# ì˜ˆì‹œ\n",
    "np.random.seed(0)  # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •\n",
    "logits = np.random.rand(1, 10)  # (1, seq_len, vocab) shapeì˜ ì„ì˜ logits\n",
    "p = 0.9  # ëˆ„ì  í™•ë¥  ê¸°ì¤€\n",
    "\n",
    "# Top-p ìƒ˜í”Œë§ ì ìš©\n",
    "selected_token = top_p_sampling(logits, p)\n",
    "print(\"Selected token index:\", selected_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "\n",
    "c = seq_len = len(prompt_tokens[0])\n",
    "mask = np.full((seq_len, seq_len), -1e9)\n",
    "mask = np.triu(mask, k=1)\n",
    "mask = np.hstack(\n",
    "    [np.zeros((seq_len, 0)), mask]\n",
    ")[None, None, :, :]\n",
    "print(mask.shape)\n",
    "\n",
    "\n",
    "state = mlmodel_int4.make_state()  # ë£¨í”„ ë‚´ì—ì„œ ìƒíƒœ ì´ˆê¸°í™”\n",
    "\n",
    "inputs = {\n",
    "    \"input_ids\": np.array(prompt_tokens, dtype=np.int32),\n",
    "    \"causal_mask\": mask,\n",
    "}\n",
    "\n",
    "logits = mlmodel_int4.predict(inputs, state = state)['logits']\n",
    "\n",
    "pre = np.argmax(logits, axis=-1)\n",
    "a.append(int(pre[0]))\n",
    "print(np.argmax(logits, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your main loop\n",
    "a = []\n",
    "for i in range(len(prompt_tokens[0])+1, len(prompt_tokens[0])+100):\n",
    "    seq_len = len(pre)\n",
    "    c = i\n",
    "    # print(seq_len)\n",
    "    mask = np.full((seq_len, seq_len), -1e9)\n",
    "    mask = np.triu(mask, k=1)\n",
    "    mask = np.hstack(\n",
    "        [np.zeros((seq_len, c)), mask]\n",
    "    )[None, None, :, :]\n",
    "    \n",
    "    inputs = {\n",
    "        \"input_ids\": np.array([pre], dtype=np.int32),\n",
    "        \"causal_mask\": mask,\n",
    "    }\n",
    "    preds = mlmodel_int4.predict(inputs, state=state)\n",
    "    logits = preds['logits']\n",
    "    logits = apply_repetition_penalty(logits, prompt_tokens[0] + a, 4.20)\n",
    "    # logits = top_p_sampling(logits, p= .9)\n",
    "    pre = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    a.append(int(pre[0]))\n",
    "    if int(pre[0]) in stop_tokens.tolist():\n",
    "        break\n",
    "\n",
    "# print(len(a))\n",
    "# print(tok.decode(prompt_tokens[0] + a))\n",
    "print(tok.decode(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(prompt_tokens[0] + a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google_gemma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
