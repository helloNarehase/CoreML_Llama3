{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import torch\n",
    "from coreml_llama import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_path = \"/Volumes/ë¬´ì œ/llama3_2/Llama3.2-1B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(128256, 2048)\n",
       "  (layers): ModuleList(\n",
       "    (0-15): 16 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (wk): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (wv): Linear(in_features=2048, out_features=512, bias=False)\n",
       "        (wo): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        (rope): RoPE()\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "        (w2): Linear(in_features=8192, out_features=2048, bias=False)\n",
       "        (w3): Linear(in_features=2048, out_features=8192, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=2048, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open(f\"{M_path}/params.json\", \"r\") as st_json:\n",
    "    params = json.load(st_json)\n",
    "params\n",
    "\n",
    "args = ModelArgs(**params)\n",
    "transformer = Transformer(args)\n",
    "\n",
    "model_pth = torch.load(f\"{M_path}/consolidated.00.pth\", map_location=\"cpu\", weights_only=True)\n",
    "transformer.load_state_dict(model_pth, strict=False)\n",
    "transformer.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokenizer, ChatFormat\n",
    "tok = Tokenizer(f\"{M_path}/tokenizer.model\")\n",
    "formatter = ChatFormat(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transformer.to(device= \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogs = [\n",
    "    [{\"role\": \"user\", \"content\": \"hello!ðŸ˜†\"}],\n",
    "]\n",
    "\n",
    "prompt_tokens = [\n",
    "    formatter.encode_dialog_prompt(dialog) for dialog in dialogs\n",
    "]\n",
    "# prompt = torch.tensor(np.array(tok.encode(\"hello world!\", bos= True, eos= False))[None, :])\n",
    "prompt = torch.tensor(prompt_tokens, device= \"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1p/m95rjf612yq76ghqqhtx5ng40000gn/T/ipykernel_50962/1709278597.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device= \"mps\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pad_id = tok.pad_id\n",
    "tokens = torch.full((1, 1000), pad_id, dtype=torch.long, device= \"mps\")\n",
    "\n",
    "for k, t in enumerate(prompt):\n",
    "    tokens[k, : len(t)] = torch.tensor(t, dtype=torch.long, device= \"mps\")\n",
    "token_logprobs = torch.zeros_like(tokens, dtype=torch.float, device= \"mps\")\n",
    "\n",
    "prev_pos = 0\n",
    "eos_reached = torch.tensor([False] * 1, device= \"mps\")\n",
    "input_text_mask = tokens != pad_id\n",
    "\n",
    "temperature = 0\n",
    "stop_tokens = torch.tensor(list(tok.stop_tokens), device= \"mps\")\n",
    "prev_pos = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = KVCache(\n",
    "    transformer.caches_shape, dtype=torch.float16, device= \"mps\"\n",
    ")\n",
    "\n",
    "for cur_pos in range(len(prompt[0]), 800):\n",
    "    seqlen = tokens[:, prev_pos:cur_pos].size(1)\n",
    "\n",
    "    mask = torch.full((seqlen, seqlen), -1e9, device= \"mps\")\n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "    mask = torch.hstack(\n",
    "        [torch.zeros((seqlen, prev_pos), device= \"mps\"), mask]\n",
    "    )[None, None, :, :]\n",
    "\n",
    "    logits = transformer.forward(tokens[:, prev_pos:cur_pos], mask, cache)\n",
    "    next_token = torch.argmax(logits[..., -1, :], dim=-1)\n",
    "\n",
    "    next_token = next_token.reshape(-1)\n",
    "    next_token = torch.where(\n",
    "        input_text_mask[:, cur_pos], tokens[:, cur_pos], next_token\n",
    "    )\n",
    "    tokens[:, cur_pos] = next_token\n",
    "    \n",
    "    eos_reached |= (~input_text_mask[:, cur_pos]) & (\n",
    "        torch.isin(next_token, stop_tokens)\n",
    "    )\n",
    "    prev_pos = cur_pos\n",
    "    if all(eos_reached):\n",
    "        break\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "hello!ðŸ˜†<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "ðŸ˜Š Hello! How's your day going so far?\n"
     ]
    }
   ],
   "source": [
    "print(tok.decode(tokens[0, :prev_pos].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Trained_Transformer(args).load_state_dict(transformer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "google_gemma2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
